# Data Science Research Tool

An AI-powered data science tool that automatically researches websites based on user input, stores content in a vector database, and generates comprehensive summaries using the OpenAI Agent API.

## Features

- ðŸŒ **Web Scraping**: Automatically extracts content from multiple websites
- ðŸ’¾ **Vector Database**: Stores scraped content in ChromaDB for efficient retrieval
- ðŸ¤– **OpenAI Agent**: Uses GPT-4 Turbo with the Assistants API to generate intelligent summaries
- ðŸ“Š **Research Analysis**: Synthesizes information from multiple sources
- ðŸ’» **CLI Interface**: Interactive command-line interface for easy usage

## Architecture

The tool consists of four main components:

1. **Web Scraper** (`web_scraper.py`): Extracts text content from websites
2. **Vector Database** (`vector_db.py`): Stores and retrieves content using ChromaDB
3. **Research Agent** (`research_agent.py`): OpenAI assistant for generating summaries
4. **Main Application** (`main.py` / `cli.py`): Coordinates the workflow

## Installation

1. Clone the repository:
```bash
git clone https://github.com/xbwei/test-copilot.git
cd test-copilot
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

3. Set up your OpenAI API key:
```bash
cp .env.example .env
# Edit .env and add your OpenAI API key
```

## Usage

### Demo Mode

Run the application with example URLs and query:

```bash
python main.py
```

This will research AI and machine learning topics using Wikipedia articles.

### Interactive Mode

Run the interactive CLI to provide your own research query and URLs:

```bash
python cli.py
```

Follow the prompts to:
1. Enter your research query
2. Provide URLs to research (one per line)
3. View the generated summary

### Programmatic Usage

You can also use the tool in your own Python code:

```python
from main import ResearchTool

# Initialize the tool
tool = ResearchTool()

# Define your research parameters
urls = [
    "https://example.com/article1",
    "https://example.com/article2"
]
query = "What are the main points discussed in these articles?"

# Perform research
summary = tool.research_websites(urls, query)
print(summary)

# Clean up
tool.cleanup()
```

## How It Works

1. **Scraping**: The tool fetches content from provided URLs using BeautifulSoup
2. **Storage**: Content is processed and stored in a ChromaDB vector database
3. **Analysis**: An OpenAI assistant analyzes the content based on your research query
4. **Summary**: The assistant generates a comprehensive summary that:
   - Identifies key themes and topics
   - Extracts important facts and data points
   - Synthesizes information from multiple sources
   - Presents findings in a clear, structured format

## Requirements

- Python 3.8+
- OpenAI API key
- Internet connection for web scraping

## Dependencies

- `openai`: OpenAI API client for GPT-4 and Assistants API
- `chromadb`: Vector database for storing and retrieving content
- `beautifulsoup4`: HTML parsing and web scraping
- `requests`: HTTP library for fetching web pages
- `python-dotenv`: Environment variable management

## Configuration

Create a `.env` file in the project root with your OpenAI API key:

```
OPENAI_API_KEY=your_openai_api_key_here
```

## Examples

### Example 1: Research AI Topics

```bash
python main.py
```

Output:
```
Research Query: What are the key concepts in AI and machine learning?
URLs to research: https://en.wikipedia.org/wiki/Artificial_intelligence, ...

[Summary generated by OpenAI Agent]
```

### Example 2: Custom Research

```bash
python cli.py
```

```
Enter your research query:
> What are the latest trends in cloud computing?

Enter URLs to research (one per line, empty line to finish):
> https://aws.amazon.com/blogs/
> https://cloud.google.com/blog/
>

[Processing and summary generation]
```

## License

MIT License - see LICENSE file for details

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## Author

Xuebin Wei